# ktbimage/main.py

import os
import json
import re
from datetime import datetime
import pytz
from io import BytesIO
import zipfile
from dotenv import load_dotenv
import requests
import subprocess
import random
from PIL import Image

# Import c√°c h√†m t·ª´ module d√πng chung
from utils.image_processing import (
    download_image,
    erase_areas,
    crop_by_coords,
    rotate_image,
    remove_background,
    remove_background_advanced,
    trim_transparent_background,
    apply_mockup,
    add_watermark,
    determine_color_from_sample_area
)
from utils.file_io import (
    load_config,
    clean_title,
    pre_clean_filename,
    should_globally_skip,
    _convert_to_gps,
    create_exif_data,
    update_total_image_count,
    find_mockup_image
)

# --- C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n ---
TOOL_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(TOOL_DIR)
PARENT_DIR = os.path.dirname(PROJECT_ROOT)

# ƒê∆∞·ªùng d·∫´n t·ªõi repo imagecrawler
CRAWLER_REPO_PATH = os.path.join(PARENT_DIR, "imagecrawler")
CRAWLER_LOG_FILE = os.path.join(CRAWLER_REPO_PATH, "imagecrawler.log")
CRAWLER_DOMAIN_DIR = os.path.join(CRAWLER_REPO_PATH, "domain")

# ƒê∆∞·ªùng d·∫´n t·ªõi c√°c t√†i nguy√™n v√† tool kh√°c
KTBIMG_INPUT_DIR = os.path.join(PROJECT_ROOT, "ktbimg", "InputImage")
CONFIG_FILE = os.path.join(PROJECT_ROOT, "config.json")
MOCKUP_DIR = os.path.join(PROJECT_ROOT, "mockup")
WATERMARK_DIR = os.path.join(PROJECT_ROOT, "watermark")
FONT_FILE = os.path.join(PROJECT_ROOT, "verdanab.ttf")

# ƒê∆∞·ªùng d·∫´n ri√™ng c·ªßa tool ktbimage
OUTPUT_DIR = os.path.join(TOOL_DIR, "OutputImage")
TOTAL_IMAGE_FILE = os.path.join(TOOL_DIR, "TotalImage.txt")
GENERATE_LOG_FILE = os.path.join(TOOL_DIR, "generate.log")

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env ·ªü th∆∞ m·ª•c g·ªëc
load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, '.env'))

# --- C√ÅC H√ÄM H·ªñ TR·ª¢ RI√äNG C·ª¶A TOOL N√ÄY ---

def cleanup_old_zips():
    if not os.path.exists(OUTPUT_DIR): return
    print("üßπ B·∫Øt ƒë·∫ßu d·ªçn d·∫πp c√°c file zip c≈©...")
    for filename in os.listdir(OUTPUT_DIR):
        if filename.endswith(".zip"):
            try:
                os.remove(os.path.join(OUTPUT_DIR, filename))
            except Exception as e:
                print(f"   - L·ªói khi x√≥a {filename}: {e}")

def commit_and_push_changes_locally():
    print("üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh commit v√† push...")
    try:
        os.chdir(PROJECT_ROOT)
        subprocess.run(['git', 'add', '.'], check=True)
        status_output = subprocess.run(['git', 'status', '--porcelain'], capture_output=True, text=True).stdout.strip()
        if not status_output:
            print("‚úÖ Kh√¥ng c√≥ thay ƒë·ªïi m·ªõi ƒë·ªÉ commit.")
            return False
        commit_message = f"Update via ktbimage tool - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        subprocess.run(['git', 'commit', '-m', commit_message], check=True)
        current_branch = subprocess.run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'], capture_output=True, text=True, check=True).stdout.strip()
        print(f"   - Commit th√†nh c√¥ng. B·∫Øt ƒë·∫ßu push l√™n nh√°nh '{current_branch}'...")
        subprocess.run(['git', 'push', 'origin', current_branch], check=True)
        print("‚úÖ Push th√†nh c√¥ng.")
        return True
    except (subprocess.CalledProcessError, FileNotFoundError) as e:
        print(f"‚ùå L·ªói trong qu√° tr√¨nh Git: {e}")
        return False

def send_telegram_log_locally():
    token, chat_id = os.getenv("TELEGRAM_BOT_TOKEN"), os.getenv("TELEGRAM_CHAT_ID")
    if not token or not chat_id:
        print("‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y bi·∫øn m√¥i tr∆∞·ªùng Telegram. B·ªè qua vi·ªác g·ª≠i log.")
        return
    try:
        with open(GENERATE_LOG_FILE, "r", encoding="utf-8") as f:
            log_content = f.read() + "\nPush successful (from PC)."
        print("‚úàÔ∏è  ƒêang g·ª≠i log t·ªõi Telegram...")
        requests.post(f"https://api.telegram.org/bot{token}/sendMessage", data={'chat_id': chat_id, 'text': log_content}, timeout=10)
        print("‚úÖ G·ª≠i log t·ªõi Telegram th√†nh c√¥ng.")
    except Exception as e:
        print(f"‚ùå L·ªói khi g·ª≠i log t·ªõi Telegram: {e}")

def write_log(urls_summary):
    """Ghi log chi ti·∫øt, bao g·ªìm c√°c lo·∫°i skip kh√°c nhau."""
    with open(GENERATE_LOG_FILE, "w", encoding="utf-8") as f:
        f.write(f"--- Summary of Last Generation ---\n")
        f.write(f"Timestamp: {datetime.now(pytz.timezone('Asia/Ho_Chi_Minh')).strftime('%Y-%m-%d %H:%M:%S')} +07\n\n")
        if not urls_summary:
            f.write("No new images were processed in this run.\n")
        else:
            for domain, counts in sorted(urls_summary.items()):
                f.write(f"Domain: {domain}\n")
                if counts.get('processed_by_mockup'):
                    for mockup, count in sorted(counts['processed_by_mockup'].items()):
                        f.write(f"  - {mockup}: {count} images\n")
                f.write(f"  - Skipped (Global): {counts['skipped_global']} images\n")
                f.write(f"  - Skipped (No Rule): {counts['skipped_no_rule']} images\n")
                f.write(f"  - Skipped (Action/Error): {counts['skipped_by_rule']} images\n")
                if counts.get('skip_file_generated'):
                    f.write(f"  - Skip File -> ktbimg: {counts['skip_file_generated']}\n")
                f.write(f"  - Total Processed URLs: {counts['total_to_process']}\n\n")
    print(f"‚úÖ Generation summary saved to {GENERATE_LOG_FILE}")


# --- H√ÄM MAIN CH√çNH ---
def main():
    print("üöÄ B·∫Øt ƒë·∫ßu quy tr√¨nh t·ª± ƒë·ªông c·ªßa KTB-IMAGE...")
    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)
    
    cleanup_old_zips()

    configs = load_config(CONFIG_FILE)
    if not configs: return
    
    defaults = configs.get("defaults", {})
    domains_configs = configs.get("domains", {})
    mockup_sets_config = configs.get("mockup_sets", {})
    exif_defaults = defaults.get("exif_defaults", {})
    title_clean_keywords = defaults.get("title_clean_keywords", [])
    global_skip_keywords = defaults.get("global_skip_keywords", [])

    try:
        with open(CRAWLER_LOG_FILE, 'r', encoding='utf-8') as f:
            log_content = f.read()
    except FileNotFoundError:
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file log t·∫°i '{CRAWLER_LOG_FILE}'."); return

    domains_to_process = {p[0].strip(): int(p[1].split()[0]) for l in log_content.splitlines() if "New Images" in l for p in [l.split(":")] if int(p[1].split()[0]) > 0}
    
    if not domains_to_process:
        print("‚úÖ Kh√¥ng c√≥ ·∫£nh m·ªõi n√†o ƒë∆∞·ª£c t√¨m th·∫•y trong log. K·∫øt th√∫c."); return

    print(f"üîé T√¨m th·∫•y {len(domains_to_process)} domain c√≥ ·∫£nh m·ªõi.")
    urls_summary = {}
    total_processed_this_run = {}
    images_for_zip = {}

    for domain, new_count in domains_to_process.items():
        print(f"\n==================== B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {new_count} ·∫£nh m·ªõi t·ª´ domain: {domain} ====================")
        
        try:
            with open(os.path.join(CRAWLER_DOMAIN_DIR, f"{domain}.txt"), 'r', encoding='utf-8') as f:
                urls_to_process = f.read().splitlines()[:new_count]
        except FileNotFoundError:
            print(f"  - ‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file URL cho domain {domain}. B·ªè qua."); continue
        
        domain_rules = sorted(domains_configs.get(domain, []), key=lambda x: len(x.get('pattern', '')), reverse=True)
        if not domain_rules:
            print(f"  - ‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y quy t·∫Øc cho domain '{domain}'."); continue

        skipped_urls_for_domain = []
        processed_by_mockup = {}
        skipped_global_count, skipped_no_rule_count, skipped_by_rule_count = 0, 0, 0

        # <<< TH√äM M·ªöI: BI·∫æN ƒê·∫æM L·ªñI V√Ä NG∆Ø·ª†NG L·ªñI >>>
        consecutive_error_count = 0
        ERROR_THRESHOLD = 5 # D·ª´ng l·∫°i n·∫øu c√≥ 5 l·ªói t·∫£i ·∫£nh li√™n ti·∫øp

        for url in urls_to_process:
            filename = os.path.basename(url)
            print(f"\n--- ƒêang x·ª≠ l√Ω: {filename} ---")
            
            if should_globally_skip(filename, global_skip_keywords):
                skipped_global_count += 1  # Ch·ªâ tƒÉng b·ªô ƒë·∫øm, kh√¥ng th√™m url v√†o danh s√°ch skip
                print(f"  - ‚è© B·ªè qua (Global): '{filename}' kh·ªõp v·ªõi t·ª´ kh√≥a skip to√†n c·ª•c.")
                continue
            
            matched_rule = next((r for r in domain_rules if r.get("pattern", "") in filename), None)
            
            if not matched_rule:
                print("  - ‚è© B·ªè qua: Kh√¥ng c√≥ quy t·∫Øc ph√π h·ª£p."); skipped_urls_for_domain.append(url); skipped_no_rule_count += 1; continue
            if matched_rule.get("action") == "skip":
                print("  - ‚è© B·ªè qua: Quy t·∫Øc c√≥ action l√† 'skip'."); skipped_urls_for_domain.append(url); skipped_by_rule_count += 1; continue

            try:
                img = download_image(url)
                if not img:
                    skipped_urls_for_domain.append(url)
                    consecutive_error_count += 1 # TƒÉng bi·∫øn ƒë·∫øm l·ªói
                    
                    # KI·ªÇM TRA N·∫æU V∆Ø·ª¢T NG∆Ø·ª†NG
                    if consecutive_error_count >= ERROR_THRESHOLD:
                        print(f"  - ‚ùå L·ªói: ƒê√£ c√≥ {consecutive_error_count} l·ªói t·∫£i ·∫£nh li√™n ti·∫øp. B·ªè qua c√°c URL c√≤n l·∫°i c·ªßa domain {domain}.")
                        break # Tho√°t kh·ªèi v√≤ng l·∫∑p c·ªßa domain n√†y
                    
                    continue # Chuy·ªÉn sang URL ti·∫øp theo
                
                # N·∫øu t·∫£i th√†nh c√¥ng, reset bi·∫øn ƒë·∫øm l·ªói v·ªÅ 0
                consecutive_error_count = 0
                # <<< B∆Ø·ªöC M·ªöI: X√ìA WATERMARK C≈® TR√äN ·∫¢NH G·ªêC >>>
                erase_zones = matched_rule.get("erase_zones")
                if erase_zones:
                    print("  - T·∫©y watermark c≈© tr√™n ·∫£nh g·ªëc...")
                    img = erase_areas(img, erase_zones)
                # <<< K·∫æT TH√öC B∆Ø·ªöC M·ªöI >>>
                sample_coords = matched_rule.get("color_sample_coords")
                angle = matched_rule.get("angle", 0)
                is_white = True
                
                if sample_coords:
                    is_white = determine_color_from_sample_area(img, sample_coords)
                    print(f"  - M√†u √°o (t·ª´ ·∫£nh g·ªëc) l√†: {'Tr·∫Øng' if is_white else 'ƒêen'}")
                    rect_coords = matched_rule.get("coords_white") if is_white else matched_rule.get("coords_black")
                    if not rect_coords: rect_coords = matched_rule.get("coords")
                else:
                    rect_coords = matched_rule.get("coords")

                if not rect_coords:
                    print("  - ‚è© B·ªè qua: Kh√¥ng t√¨m th·∫•y t·ªça ƒë·ªô ph√π h·ª£p."); skipped_urls_for_domain.append(url); skipped_by_rule_count += 1; continue
                
                initial_crop = crop_by_coords(img, rect_coords)
                if not initial_crop:
                    skipped_urls_for_domain.append(url); continue

                if not sample_coords: # Fallback logic
                    try:
                        pixel = initial_crop.getpixel((1, initial_crop.height - 2))
                        is_white = sum(pixel[:3]) / 3 > 128
                        print(f"  - M√†u √°o (t·ª´ ·∫£nh crop) l√†: {'Tr·∫Øng' if is_white else 'ƒêen'}")
                    except IndexError:
                        is_white = True
                
                if (matched_rule.get("skipWhite") and is_white) or (matched_rule.get("skipBlack") and not is_white):
                    print("  - ‚è© B·ªè qua theo quy t·∫Øc skip m√†u."); skipped_urls_for_domain.append(url); skipped_by_rule_count += 1; continue
                
                # C√°ch 1: D√πng h√†m c≈©, nhanh h∆°n, ch·∫•t l∆∞·ª£ng ti√™u chu·∫©n
                #bg_removed = remove_background(initial_crop)

                # C√°ch 2: D√πng h√†m m·ªõi, ch·∫≠m h∆°n, ch·∫•t l∆∞·ª£ng v∆∞·ª£t tr·ªôi
                bg_removed = remove_background_advanced(initial_crop)
                final_design = rotate_image(bg_removed, angle)
                trimmed_img = trim_transparent_background(final_design)
                if not trimmed_img:
                    print("  - ‚ö†Ô∏è C·∫£nh b√°o: ·∫¢nh tr·ªëng sau khi x·ª≠ l√Ω."); skipped_urls_for_domain.append(url); skipped_by_rule_count += 1; continue
                
                mockup_names_to_use = matched_rule.get("mockup_sets_to_use", [])
                if not mockup_names_to_use:
                    print("  - ‚è© B·ªè qua: Quy t·∫Øc kh√¥ng ch·ªâ ƒë·ªãnh 'mockup_sets_to_use'."); skipped_urls_for_domain.append(url); skipped_by_rule_count += 1; continue

                for mockup_name in mockup_names_to_use:
                    mockup_config = mockup_sets_config.get(mockup_name)
                    if not mockup_config: print(f"  - ‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y config cho mockup '{mockup_name}'."); continue
                    
                    mockup_path = find_mockup_image(MOCKUP_DIR, mockup_name, "white" if is_white else "black")
                    if not mockup_path: print(f"  - ‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y file ·∫£nh mockup cho '{mockup_name}'."); continue
                    
                    mockup_img = Image.open(mockup_path)
                    final_mockup = apply_mockup(trimmed_img, mockup_img, mockup_config.get("coords"))
                    watermark_desc = mockup_config.get("watermark_text")
                    final_mockup_with_wm = add_watermark(final_mockup, watermark_desc, WATERMARK_DIR, FONT_FILE)

                    # --- LOGIC T·∫†O T√äN FILE (ƒê√É C·∫¨P NH·∫¨T) ---
                    base_filename = os.path.splitext(filename)[0]

                    # B∆Ø·ªöC PH·ª§: TI·ªÄN X·ª¨ L√ù T√äN FILE N·∫æU C√ì REGEX
                    pre_clean_pattern = matched_rule.get("pre_clean_regex")
                    if pre_clean_pattern:
                        print(f"  - √Åp d·ª•ng pre_clean_regex: '{pre_clean_pattern}'")
                        base_filename = pre_clean_filename(base_filename, pre_clean_pattern)

                    # B∆Ø·ªöC CH√çNH: D·ªåN D·∫∏P T√äN FILE B·∫∞NG KEYWORDS
                    cleaned_title = clean_title(base_filename, title_clean_keywords)
                    prefix = mockup_config.get("title_prefix_to_add", "")
                    suffix = mockup_config.get("title_suffix_to_add", "")
                    final_filename_base = f"{prefix} {cleaned_title} {suffix}".strip().replace('  ', ' ')
                    save_format, ext = ("WEBP", ".webp") if defaults.get("global_output_format", "webp") == "webp" else ("JPEG", ".jpg")
                    final_filename = f"{final_filename_base}{ext}"
                    
                    image_to_save = final_mockup_with_wm.convert('RGB')
                    exif_bytes = create_exif_data(mockup_name, final_filename, exif_defaults)
                    
                    img_byte_arr = BytesIO()
                    image_to_save.save(img_byte_arr, format=save_format, quality=90, exif=exif_bytes)
                    
                    images_for_zip.setdefault(mockup_name, {}).setdefault(domain, []).append((final_filename, img_byte_arr.getvalue()))
                    processed_by_mockup[mockup_name] = processed_by_mockup.get(mockup_name, 0) + 1
            
            except Exception as e:
                print(f"  - ‚ùå L·ªói nghi√™m tr·ªçng khi x·ª≠ l√Ω ·∫£nh {url}: {e}")
                skipped_urls_for_domain.append(url)
                skipped_by_rule_count += 1

        skip_file_name = None
        if skipped_urls_for_domain:
            if not os.path.exists(KTBIMG_INPUT_DIR): os.makedirs(KTBIMG_INPUT_DIR)
            timestamp = datetime.now(pytz.timezone('Asia/Ho_Chi_Minh')).strftime('%Y%m%d%H%M%S')
            total_skipped = len(skipped_urls_for_domain)
            skip_file_name = f"{domain}.{total_skipped}.{timestamp}.txt"
            skip_file_path = os.path.join(KTBIMG_INPUT_DIR, skip_file_name)
            with open(skip_file_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(skipped_urls_for_domain))
            print(f"üìù ƒê√£ t·∫°o file skip '{skip_file_name}' v√† ƒë·∫©y v√†o Input c·ªßa KTBIMG.")

        urls_summary[domain] = {
            'processed_by_mockup': processed_by_mockup, 'skipped_global': skipped_global_count,
            'skipped_no_rule': skipped_no_rule_count, 'skipped_by_rule': skipped_by_rule_count,
            'skip_file_generated': skip_file_name, 'total_to_process': new_count
        }
        for mockup, count in processed_by_mockup.items():
            total_processed_this_run[mockup] = total_processed_this_run.get(mockup, 0) + count

    if images_for_zip:
        for mockup_name, domains_dict in images_for_zip.items():
            for domain_name, image_list in domains_dict.items():
                if not image_list: continue
                now_vietnam = datetime.now(pytz.timezone('Asia/Ho_Chi_Minh'))
                zip_filename = f"{mockup_name}.{domain_name.split('.')[0]}.{now_vietnam.strftime('%Y%m%d_%H%M%S')}.{len(image_list)}.zip"
                zip_path = os.path.join(OUTPUT_DIR, zip_filename)
                print(f"üì¶ ƒêang t·∫°o file: {zip_path} v·ªõi {len(image_list)} ·∫£nh.")
                with zipfile.ZipFile(zip_path, 'w') as zf:
                    for filename, data in image_list:
                        zf.writestr(filename, data)

    write_log(urls_summary)
    update_total_image_count(TOTAL_IMAGE_FILE, total_processed_this_run)
    print("\n‚úÖ Ho√†n th√†nh t·∫°o file zip v√† log.")

    #if commit_and_push_changes_locally():
    #    send_telegram_log_locally()
    commit_and_push_changes_locally()
    send_telegram_log_locally()
    print("\nüéâ Quy tr√¨nh ƒë√£ ho√†n t·∫•t! üéâ")

if __name__ == "__main__":
    main()